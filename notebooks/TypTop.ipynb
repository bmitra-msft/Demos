{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Demonstration of Typical and Topical Similarities in Neural Text Embedding Spaces\n",
    "\n",
    "The notion of similarity between two pieces of text in a learned embedding space is a function of how the embedding model is trained. Is _yale_ closer to _harvard_, or to _alumni_? Among the many possible notions of relatedness type-based (_yale_ to _harvard_) and topic-based (_yale_ to _alumni_) similarities are more popularly studied in the literature. The [DESM paper](https://arxiv.org/abs/1602.01137) (2016) referred to these two relationships as _Typical_ and _Topical_ similarities, respectively, in the context of the popular [word2vec](https://arxiv.org/abs/1301.3781.pdf) model. However, the idea of _Typical_ and _Topical_ similarities goes at least as far back as [Saussure](https://en.wikipedia.org/wiki/Ferdinand_de_Saussure) who referred to them under the (dare we say) slightly harder to enunciate names of [_Syntagmatic_ and _Paradigmatic_](https://en.wikipedia.org/wiki/Course_in_General_Linguistics#Syntagmatic_and_paradigmatic_relations) relations. Other, recent works (e.g., [Sun et al.](http://www.aclweb.org/anthology/P15-1014)) have also distinguished between these two relationships in the context of word embedding spaces.\n",
    "\n",
    "Obviously, the notion of _Typical_ and _Topical_ similarities goes beyond just words. This demonstration explores the same relationships in the context of short-text embeddings. Both the _Topical_ and the _Typical_ model architectures in this demo are based on the [CDSSM model](http://dl.acm.org/citation.cfm?id=2661935). However, while the _Topical_ model is trained on query-document pairs as specified by the original paper, the _Typical_ model is trained on query prefix-suffix pairs as proposed by [Mitra and Craswell](http://dl.acm.org/citation.cfm?id=2806599).\n",
    "\n",
    "Last but not the least, we also demonstrate that the _Typical_-CDSSM model in particular is capable of performing analogies over short-text using simple vector algebra in line with what was reported by [Mitra](http://dl.acm.org/citation.cfm?id=2767702).\n",
    "\n",
    "## Let's begin..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import operator\n",
    "import pprint\n",
    "import numpy as np\n",
    "import cntk as C\n",
    "from cntk import graph\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "C.set_default_device(C.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Sample:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.source = \"\"\n",
    "        self.targets = []\n",
    "    \n",
    "class DataReader:\n",
    "    max_words = 10\n",
    "    \n",
    "    def __init__(self, data_file, ngraphs_file, num_negs):\n",
    "        self.__load_ngraphs(ngraphs_file)\n",
    "        self.data_file = open(data_file, mode='r')\n",
    "        self.num_negs = num_negs\n",
    "    \n",
    "    def __load_ngraphs(self, filename):\n",
    "        self.ngraphs = {}\n",
    "        self.max_ngraph_len = 0\n",
    "        with open(filename, mode='r') as f:\n",
    "            reader = csv.reader(f, delimiter='\\t')\n",
    "            for row in reader:\n",
    "                self.ngraphs[row[0]] = int(row[1]) - 1\n",
    "                self.max_ngraph_len = max(self.max_ngraph_len, len(row[0]))\n",
    "        self.num_ngraphs = len(self.ngraphs)\n",
    "\n",
    "    def __read_samples(self, num_samples):\n",
    "        labels = np.zeros((num_samples, self.num_negs+1))\n",
    "        samples_qd = []\n",
    "        samples_ps = []\n",
    "        mb_size = 0\n",
    "        for i in range(num_samples):\n",
    "            query = \"\"\n",
    "            doc = \"\"\n",
    "            query_words = []\n",
    "            num_words = 0\n",
    "            while num_words < 2:\n",
    "                row = self.data_file.readline()\n",
    "                if row == \"\":\n",
    "                    self.data_file.seek(0)\n",
    "                else:\n",
    "                    row = re.sub('[^0-9a-z\\t]+', ' ', row.lower())\n",
    "                    cols = row.split('\\t')\n",
    "                    query = cols[0]\n",
    "                    doc = cols[1]\n",
    "                    query_words = query.split(' ')\n",
    "                    num_words = len(query_words)\n",
    "            if num_words < 2:\n",
    "                break\n",
    "            num_words_p = random.randint(1, num_words-1)\n",
    "            prefix = ' '.join(query_words[:num_words_p])\n",
    "            suffix = ' '.join(query_words[num_words_p:])\n",
    "            \n",
    "            curr_sample_qd = Sample()\n",
    "            curr_sample_qd.source = query\n",
    "            curr_sample_qd.targets.append(doc)\n",
    "            samples_qd.append(curr_sample_qd)\n",
    "            \n",
    "            curr_sample_ps = Sample()\n",
    "            curr_sample_ps.source = prefix\n",
    "            curr_sample_ps.targets.append(suffix)\n",
    "            samples_ps.append(curr_sample_ps)\n",
    "\n",
    "            labels[i][0] = 1\n",
    "            mb_size += 1\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            for j in range(1, self.num_negs+1):\n",
    "                samples_qd[i].targets.append(samples_qd[(i+j)%num_samples].targets[0])\n",
    "                samples_ps[i].targets.append(samples_ps[(i+j)%num_samples].targets[0])\n",
    "                \n",
    "        return samples_qd, samples_ps, labels, mb_size\n",
    "        \n",
    "    def __get_ngraph_features(self, samples):\n",
    "        features_src = np.zeros((len(samples), self.num_ngraphs, self.max_words))\n",
    "        features_tgts = np.zeros((len(samples), self.num_negs+1, self.num_ngraphs, self.max_words))\n",
    "        for sample_idx, sample in enumerate(samples):\n",
    "            # loop over source and targets -- tgt_idx = 0 corresponds to source \n",
    "            for tgt_idx in range(len(sample.targets)+1):\n",
    "                tgt = sample.source if tgt_idx == 0 else sample.targets[tgt_idx-1]\n",
    "                for w_idx, word in enumerate(tgt.split()):\n",
    "                    token = '#' + word + '#'\n",
    "                    token_len = len(token)\n",
    "                    for i in range(token_len):\n",
    "                        for j in range(0, self.max_ngraph_len):\n",
    "                            if i+j < token_len:\n",
    "                                ngraph_idx = self.ngraphs.get(token[i:i+j])\n",
    "                                if ngraph_idx != None:\n",
    "                                    if tgt_idx == 0:\n",
    "                                        features_src[sample_idx, ngraph_idx, min(w_idx, self.max_words-1)] += 1\n",
    "                                    else:\n",
    "                                        features_tgts[sample_idx, tgt_idx-1, ngraph_idx, min(w_idx, self.max_words-1)] += 1\n",
    "        return features_src, features_tgts\n",
    "\n",
    "    def get_minibatch(self, num_samples):\n",
    "        samples_qd, samples_ps, labels, mb_size = self.__read_samples(num_samples)\n",
    "        features_qd_src, features_qd_tgts = self.__get_ngraph_features(samples_qd)\n",
    "        features_ps_src, features_ps_tgts = self.__get_ngraph_features(samples_ps)\n",
    "        return features_qd_src, features_qd_tgts, features_ps_src, features_ps_tgts, labels, mb_size\n",
    "\n",
    "    def get_test_minibatches(self, candidates_file, mb_size):        \n",
    "        empty_targets = []\n",
    "        for i in range(self.num_negs+1):\n",
    "            empty_targets.append(\"\")\n",
    "        \n",
    "        query = \"\"\n",
    "        with open(candidates_file, mode='r') as f:\n",
    "            while True:\n",
    "                samples = []\n",
    "                candidates = []\n",
    "                curr_mb_size = 0\n",
    "                while curr_mb_size < mb_size:\n",
    "                    query = f.readline().strip()\n",
    "                    if query == \"\":\n",
    "                        break\n",
    "                    curr_sample = Sample()\n",
    "                    curr_sample.source = query\n",
    "                    curr_sample.targets = empty_targets\n",
    "                    samples.append(curr_sample)\n",
    "                    candidates.append(query)\n",
    "                    curr_mb_size += 1\n",
    "                features_src = None\n",
    "                features_tgts = None\n",
    "                if curr_mb_size != 0:\n",
    "                    features_src, features_tgts = self.__get_ngraph_features(samples)\n",
    "                yield features_src, candidates, curr_mb_size\n",
    "                if query == \"\":\n",
    "                        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CDSSM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_model(num_words, num_hidden_nodes, convolutional):\n",
    "    if convolutional:\n",
    "        word_window_size = 3\n",
    "        pooling_kernel_width = num_words - word_window_size + 1 # = 8\n",
    "        return C.Sequential ([\n",
    "                C.Convolution((word_window_size, 1), num_hidden_nodes, activation=C.tanh, strides=(1, 1), pad=False),\n",
    "                C.MaxPooling((pooling_kernel_width, 1), strides=(1, 1), pad=False),\n",
    "                #C.Dense(num_hidden_nodes, activation=C.tanh),\n",
    "                C.Dense(num_hidden_nodes, activation=C.tanh)])\n",
    "    else:\n",
    "        return C.Sequential ([\n",
    "                C.Dense(num_hidden_nodes, activation=C.tanh),\n",
    "                #C.Dense(num_hidden_nodes, activation=C.tanh),\n",
    "                C.Dense(num_hidden_nodes, activation=C.tanh)])\n",
    "    \n",
    "def dssm(features_src, features_tgts, num_ngraphs, num_words, num_negs, num_hidden_nodes, convolutional):\n",
    "    const_gamma = C.constant(10.0, shape=(1, 1))\n",
    "    embed_src   = get_embedding_model(num_words, num_hidden_nodes, convolutional)\n",
    "    embed_tgt   = get_embedding_model(num_words, num_hidden_nodes, convolutional)\n",
    "    net_src     = C.reshape(features_src, (num_ngraphs, num_words, 1))\n",
    "    net_src     = embed_src(net_src)\n",
    "    net_src     = C.alias(net_src, name='source_embedding')\n",
    "    net_tgts    = [C.slice(features_tgts, 0, idx, idx+1) for idx in range(0, num_negs+1)]\n",
    "    net_tgts    = [C.reshape(net_tgt, (num_ngraphs, num_words, 1)) for net_tgt in net_tgts]\n",
    "    net_tgts    = [embed_tgt(net_tgt) for net_tgt in net_tgts]\n",
    "    net_tgts    = [C.cosine_distance(net_src, net_tgt) for net_tgt in net_tgts]\n",
    "    net_tgts    = C.splice(net_tgts)\n",
    "    net_tgts    = C.element_times(net_tgts, const_gamma)\n",
    "\n",
    "    return net_tgts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_file, ngraphs_file, num_negs, minibatch_size, minibatches_per_epoch, epochs, learning_rate, num_hidden_nodes, convolutional, test_query):\n",
    "    \n",
    "    # initialize train data readers\n",
    "    reader = DataReader(train_file, ngraphs_file, num_negs)\n",
    "       \n",
    "    # input variables denoting the features and label data\n",
    "    features_src  = C.input_variable((reader.num_ngraphs, reader.max_words), np.float32)\n",
    "    features_tgts = C.input_variable((reader.num_negs+1, reader.num_ngraphs, reader.max_words), np.float32)\n",
    "    labels        = C.input_variable((reader.num_negs+1), np.float32)\n",
    "\n",
    "    # Instantiate the Topical model and specify loss function\n",
    "    z_top   = dssm(features_src, features_tgts, reader.num_ngraphs, reader.max_words, reader.num_negs, num_hidden_nodes, convolutional)\n",
    "    ce_top  = C.cross_entropy_with_softmax(z_top, labels)\n",
    "    pe_top  = C.classification_error(z_top, labels)\n",
    "    emb_top = C.combine([z_top.find_by_name(\"source_embedding\").owner])\n",
    "\n",
    "    # Instantiate the Typical model and specify loss function\n",
    "    z_typ   = dssm(features_src, features_tgts, reader.num_ngraphs, reader.max_words, reader.num_negs, num_hidden_nodes, convolutional)\n",
    "    ce_typ  = C.cross_entropy_with_softmax(z_typ, labels)\n",
    "    pe_typ  = C.classification_error(z_typ, labels)\n",
    "    emb_typ = C.combine([z_typ.find_by_name(\"source_embedding\").owner])\n",
    "\n",
    "    # Instantiate the trainer objects to drive the model training\n",
    "    lr_per_minibatch = C.learning_rate_schedule(learning_rate, C.UnitType.minibatch)\n",
    "    trainer_top = C.Trainer(z_top, ce_top, pe_top, [C.sgd(z_top.parameters, lr=lr_per_minibatch)])\n",
    "    trainer_typ = C.Trainer(z_typ, ce_typ, pe_typ, [C.sgd(z_typ.parameters, lr=lr_per_minibatch)])\n",
    "\n",
    "    pp_top = C.ProgressPrinter(freq=10, tag='Training', gen_heartbeat=False, num_epochs=epochs)\n",
    "    pp_typ = C.ProgressPrinter(freq=10, tag='Training', gen_heartbeat=False, num_epochs=epochs)\n",
    "\n",
    "    for i in range(epochs+1):\n",
    "        \n",
    "        print('\\nafter {} epoch(s)'.format(i))\n",
    "        display_neighbours(emb_top, emb_typ, reader, minibatch_size)\n",
    "        \n",
    "        if i < epochs:\n",
    "            # train epoch\n",
    "            for j in range(minibatches_per_epoch):\n",
    "                train_features_query, train_features_docs, train_features_prefix, train_features_suffixes, train_labels, actual_mb_size = reader.get_minibatch(minibatch_size)\n",
    "                trainer_top.train_minibatch({features_src : train_features_query, features_tgts : train_features_docs, labels : train_labels})\n",
    "                trainer_typ.train_minibatch({features_src : train_features_prefix, features_tgts : train_features_suffixes, labels : train_labels})\n",
    "                pp_top.update_with_trainer(trainer_top, with_metric=True)\n",
    "                pp_typ.update_with_trainer(trainer_typ, with_metric=True)\n",
    "\n",
    "            pp_top.epoch_summary(with_metric=True)\n",
    "            pp_typ.epoch_summary(with_metric=True)\n",
    "\n",
    "    return z_top, z_typ\n",
    "\n",
    "def display_neighbours(emb_top, emb_typ, reader, minibatch_size):\n",
    "    embeddings_top = {}\n",
    "    scores_top = {}\n",
    "    embeddings_typ = {}\n",
    "    scores_typ = {}\n",
    "    test_minibatches = reader.get_test_minibatches(\"data\\\\queries-wiki.txt\", minibatch_size)\n",
    "    while True:\n",
    "        test_features, candidates, actual_mb_size = next(test_minibatches)\n",
    "        if actual_mb_size > 0:\n",
    "            result_top = emb_top.eval({emb_top.arguments[0] : test_features})\n",
    "            result_top = result_top.reshape((actual_mb_size, num_hidden_nodes))\n",
    "            result_typ = emb_typ.eval({emb_typ.arguments[0] : test_features})\n",
    "            result_typ = result_typ.reshape((actual_mb_size, num_hidden_nodes))\n",
    "            for k in range(len(candidates)):\n",
    "                embeddings_top[candidates[k]] = result_top[k]\n",
    "                embeddings_typ[candidates[k]] = result_typ[k]\n",
    "        if actual_mb_size < minibatch_size:\n",
    "            break\n",
    "    test_embedding_top = embeddings_top[test_query]\n",
    "    test_embedding_typ = embeddings_typ[test_query]\n",
    "    for k,v in embeddings_top.items():\n",
    "        if k != test_query:\n",
    "            scores_top[k] = cosine_sim(test_embedding_top, v)\n",
    "    for k,v in embeddings_typ.items():\n",
    "        #if k != test_query:\n",
    "        scores_typ[k] = cosine_sim(test_embedding_typ, v)\n",
    "    sorted_candidates_top = sorted(scores_top.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sorted_candidates_typ = sorted(scores_typ.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    display(HTML('<table><tr><th>Topical neighbours</th><th>Typical neighbours</th></tr><tr>{}</tr></table>'.format('</tr><tr>'.join('<td>{}</td><td>{}</td>'.format(sorted_candidates_top[k][0], sorted_candidates_typ[k][0])for k in range(10)))))\n",
    "    \n",
    "def cosine_sim(a, b):\n",
    "    d = dot_product(a, b);\n",
    "    na = max(1e-20, norm(a));\n",
    "    nb = max(1e-20, norm(b));\n",
    "    return (d / (na * nb) + 1) / 2;\n",
    "\n",
    "def dot_product(a, b):\n",
    "    result = 0;\n",
    "    for i in range(len(a)):\n",
    "        result += (a[i] * b[i])\n",
    "    return result;\n",
    "    \n",
    "def norm(a):\n",
    "    return math.sqrt(dot_product(a, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minibatch_size = 1024\n",
    "minibatches_per_epoch = 128\n",
    "epochs = 8\n",
    "train_file = \"data\\\\query-titles.txt\"\n",
    "ngraphs_file = \"data\\\\ngraphs.txt\"\n",
    "num_negs = 4\n",
    "learning_rate = 0.5\n",
    "num_hidden_nodes = 128\n",
    "convolutional = False\n",
    "test_query = \"seattle\"\n",
    "\n",
    "model_top, model_typ = train(train_file, ngraphs_file, num_negs, minibatch_size, minibatches_per_epoch, epochs, learning_rate, num_hidden_nodes, convolutional, test_query)\n",
    "print('bye!')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [cntk-py34]",
   "language": "python",
   "name": "Python [cntk-py34]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
